{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122901,
     "status": "ok",
     "timestamp": 1744700630784,
     "user": {
      "displayName": "Syam chand Yannakula",
      "userId": "16071886527845320126"
     },
     "user_tz": -330
    },
    "id": "-WXtxT4lAhAO",
    "outputId": "da3602c9-7146-4bab-91c1-fd33689e52fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m673.5/673.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h--- Required packages installation attempted (YouTube Focus) ---\n",
      "Using python-telegram-bot version: 22.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Installations (YouTube Focused)\n",
    "!pip install nltk spacy -q # Keep spacy only if actively used, otherwise remove\n",
    "!pip install requests beautifulsoup4 youtube-transcript-api google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 -q # Google/YT APIs\n",
    "!pip install scikit-learn -q # For TF-IDF\n",
    "!pip install transformers[torch] sentencepiece -q # Transformers + PyTorch backend\n",
    "!pip install python-telegram-bot --upgrade -q # Telegram Bot library\n",
    "!pip install nest_asyncio -q # For Colab async compatibility\n",
    "!pip install vaderSentiment -q # Keep VADER for potential sentence scoring or fallback\n",
    "\n",
    "print(\"--- Required packages installation attempted (YouTube Focus) ---\")\n",
    "\n",
    "# Verify telegram bot version\n",
    "import telegram\n",
    "print(f\"Using python-telegram-bot version: {telegram.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3694,
     "status": "ok",
     "timestamp": 1744700634481,
     "user": {
      "displayName": "Syam chand Yannakula",
      "userId": "16071886527845320126"
     },
     "user_tz": -330
    },
    "id": "uzXIF3cLLwOR",
    "outputId": "e85a2831-caee-4786-a1c4-235a58b33a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK resources (punkt, stopwords, vader_lexicon)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data/...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "ERROR:__main__:ERROR downloading/loading NLTK data: name 'stopwords' is not defined. Some features might fail.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK downloads completed/verified.\n",
      "--- NLTK setup complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: NLTK Downloads & Core Imports (YouTube Focus)\n",
    "import nltk\n",
    "# import spacy # Only if you actually use it in YT functions\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "import pickle\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# --- Setup Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- NLTK Downloads ---\n",
    "print(\"Downloading NLTK resources (punkt, stopwords, vader_lexicon)...\")\n",
    "try:\n",
    "    # Force download punkt for Colab robustness\n",
    "    nltk.download('punkt', download_dir='/root/nltk_data/', force=True, quiet=False, raise_on_error=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('vader_lexicon', quiet=True) # Keep VADER lexicon\n",
    "    print(\"NLTK downloads completed/verified.\")\n",
    "    stop_words_nltk = set(stopwords.words('english'))\n",
    "    logger.info(\"NLTK stopwords loaded.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"ERROR downloading/loading NLTK data: {e}. Some features might fail.\")\n",
    "    stop_words_nltk = set() # Fallback\n",
    "\n",
    "# --- spaCy Download (Remove if not used) ---\n",
    "# model_name_spacy = 'en_core_web_sm'\n",
    "# if not spacy.util.is_package(model_name_spacy):\n",
    "#     print(f\"Downloading spaCy model: {model_name_spacy}...\")\n",
    "#     spacy.cli.download(model_name_spacy, \"-q\")\n",
    "\n",
    "# Apply nest_asyncio for Colab compatibility\n",
    "nest_asyncio.apply()\n",
    "logger.info(\"nest_asyncio applied.\")\n",
    "\n",
    "print(\"--- NLTK setup complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448,
     "referenced_widgets": [
      "eca7ae5ffc754a30ba89fef4b9a72021",
      "72e307db166b4b919cc2f530fed290c5",
      "bb4f0973bfa14c44ac2dc87ba35f8731",
      "445a0e3a672a4554b6a3caa3040897fb",
      "ce63642dfd0947bbac05ae74b8b5bfd6",
      "4b5a65743d72411894d41de9773701ce",
      "fc375518ba05441a8728275e897178b2",
      "855706c0f96346a8b413c14b4efe9d6b",
      "1dab8f1a1a8c4e41a1a1ed4356c13fa4",
      "d2ef4548b6664643b2753f975b151f98",
      "d32d6134f3b64209b8451c1e85f87bba",
      "c7c87b5e69f949939aeb7bf89d04ec7c",
      "8c2afbb8dcfe4029aaaa6b4205fbdb30",
      "2e08e98f57f8483cbd7205d843d5d4d6",
      "eab7edc9702c42d9b4d9f2ab70278fa1",
      "4e9237d78fd94c47b0e5854609f57bf9",
      "c904a6f487834956937f6eb79e492165",
      "25ce6a8fd9dd4d1190321bad0ee3d9e4",
      "492df9212bc24d7fbb8b2376c4829f72",
      "ab09be4060dd424f8ac33e925d314c5f",
      "bca9fea9ed7a4df0918c50b61bb02e4a",
      "bb663c3a6604447aa5a61fb13cf9c3c3",
      "6cefe16dd3114344bf288284994c4b6f",
      "98aaf80f328441db8a3922578c7edc9a",
      "5ac4d980040b43ed94f1178558d467a6",
      "3f183aa3050c43ce9c7dd05893cbceca",
      "ff932a2265c24257b19a510e67355823",
      "745f4855086845ca834766035f9d419c",
      "f967393283dd49958af5e0124c9f0338",
      "0238813eadaa4e0cb9650c1d5b611e3c",
      "358530789af74bc2a7e1b88c8b9f8cb9",
      "38c8e8cbbe6a41749b5522b16ec76d05",
      "1b8c7abf95c149f99ebe182d2d3b26d7",
      "a2481be046b84aa08b15b9f87d76727a",
      "29fe8e6e459a4908ad3d062e3c9546a0",
      "c74a815922bd4ca780a0d2843fef008a",
      "8a0ff8e068974e7385b58f4e785452f8",
      "36237d7775d1420f90ce178d26f2d798",
      "b9769b90d9ca4219a0977baa9dc19d88",
      "f7c7a9783c9149d5b32efee230d77779",
      "89a345ec17f54fb09b5f09918b2ca805",
      "d535fcde0eda449cb5f1e4bfae0cb6fe",
      "7a3bb6e5a0314546b86b4d9bfb86d415",
      "7a1a7cf60bb74f359131bab042e76504",
      "e2e884339d4f41b5b78c2ca357272eda",
      "4a99f01d9ebc4fc4960730dad0769298",
      "9236480ed3924e519c0bd03510ca4c70",
      "32bd40e20fb44f69a76a1ace12f79edc",
      "47bcc3a6e110488eba9226276cba8965",
      "1b2118ff48ff425b843d742fc971ae3a",
      "9d795eb1f5c14252b821961f7547eb88",
      "d22c4275b1d24c56a04722ad0ebde652",
      "8dcf501ba0e54a8cbf7257b150c2c042",
      "1b6f546e144b44429103d7bf2b133fb9",
      "fd8e458b2b0a44a78817093c6239c9c6",
      "63898e3d613349129a5c24307b7a6f27",
      "61805f6db81340f195130953a3132de4",
      "40e9c7f8b98c487cbf61065bd2e58595",
      "16de6fe5097044d19e17ca10a33f94fc",
      "c3a0aab8161b41d4a55082c0062cfe26",
      "5dee7a79592f4ce6b50635ff22a591f9",
      "d685e267cc2a490bb015b87086eb9108",
      "53a3b8048b564a28a4dd225cafa9fc46",
      "9a0a1a57682b4f41abd12a3f47788eeb",
      "a939b0a63bcb4d6fb5ee0f58925c948c",
      "df3c5376f24a48faada33d3eec07f856"
     ]
    },
    "executionInfo": {
     "elapsed": 32677,
     "status": "ok",
     "timestamp": 1744700667162,
     "user": {
      "displayName": "Syam chand Yannakula",
      "userId": "16071886527845320126"
     },
     "user_tz": -330
    },
    "id": "mvfCs_x0LwQn",
    "outputId": "3e86ba49-7a0a-4aad-8391-c7e58582dba6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca7ae5ffc754a30ba89fef4b9a72021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c87b5e69f949939aeb7bf89d04ec7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cefe16dd3114344bf288284994c4b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2481be046b84aa08b15b9f87d76727a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e884339d4f41b5b78c2ca357272eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63898e3d613349129a5c24307b7a6f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration, Globals, and Initializations Complete (YouTube Focus) ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configuration, Global Variables & Initializations (YouTube Focused)\n",
    "\n",
    "# --- Library Imports needed for this cell ---\n",
    "from google.colab import userdata # For Colab secrets\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# --- YouTube Configuration ---\n",
    "YOUTUBE_CATEGORY_MAP = {\n",
    "    \"1\": \"Film & Animation\", \"2\": \"Autos & Vehicles\", \"10\": \"Music\", \"15\": \"Pets & Animals\",\n",
    "    \"17\": \"Sports\", \"19\": \"Travel & Events\", \"20\": \"Gaming\", \"22\": \"People & Blogs\",\n",
    "    \"23\": \"Comedy\", \"24\": \"Entertainment\", \"25\": \"News & Politics\", \"26\": \"Howto & Style\",\n",
    "    \"27\": \"Education\", \"28\": \"Science & Technology\", \"29\": \"Nonprofits & Activism\",\n",
    "    \"30\": \"Movies\", \"43\": \"Shows\"\n",
    "}\n",
    "\n",
    "# --- Global Variables (Placeholders) ---\n",
    "youtube_service = None\n",
    "vader_analyzer = None\n",
    "sentiment_pipeline_yt = None # Pipeline for YouTube comment sentiment\n",
    "sentence_tokenizer = None      # Explicitly loaded NLTK sentence tokenizer\n",
    "TELEGRAM_BOT_TOKEN = None\n",
    "YOUTUBE_API_KEY = None\n",
    "\n",
    "# --- Load Secrets ---\n",
    "logger.info(\"Loading secrets...\")\n",
    "try:\n",
    "    TELEGRAM_BOT_TOKEN = userdata.get('TELEGRAM_BOT_TOKEN')\n",
    "    YOUTUBE_API_KEY = userdata.get('YOUTUBE_API_KEY')\n",
    "    if not TELEGRAM_BOT_TOKEN: logger.error(\"TELEGRAM_BOT_TOKEN not found in Colab Secrets.\")\n",
    "    else: logger.info(\"Telegram Bot Token loaded.\")\n",
    "    if not YOUTUBE_API_KEY: logger.warning(\"YOUTUBE_API_KEY not found. YouTube analysis may fail.\")\n",
    "    else: logger.info(\"YouTube API Key loaded.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading secrets: {e}\")\n",
    "\n",
    "# --- Initialize VADER Analyzer ---\n",
    "logger.info(\"Initializing VADER Analyzer...\")\n",
    "try:\n",
    "    vader_analyzer = SentimentIntensityAnalyzer()\n",
    "    logger.info(\"VADER Analyzer initialized.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize VADER: {e}\")\n",
    "\n",
    "# --- Load NLTK Sentence Tokenizer Explicitly ---\n",
    "logger.info(\"Loading NLTK Sentence Tokenizer explicitly...\")\n",
    "punkt_english_pickle_path = '/root/nltk_data/tokenizers/punkt/english.pickle'\n",
    "try:\n",
    "    if os.path.exists(punkt_english_pickle_path):\n",
    "        with open(punkt_english_pickle_path, 'rb') as f:\n",
    "            sentence_tokenizer = pickle.load(f)\n",
    "        logger.info(\"Explicitly loaded sentence tokenizer.\")\n",
    "    else:\n",
    "        logger.error(f\"Sentence tokenizer file not found: {punkt_english_pickle_path}. Ensure Cell 2 ran.\")\n",
    "except Exception as e_load:\n",
    "     logger.error(f\"Failed to explicitly load sentence tokenizer: {e_load}\")\n",
    "\n",
    "# --- Initialize YouTube API Service ---\n",
    "logger.info(\"Initializing YouTube API Service...\")\n",
    "if YOUTUBE_API_KEY:\n",
    "    try:\n",
    "        youtube_service = build(\"youtube\", \"v3\", developerKey=YOUTUBE_API_KEY, cache_discovery=False)\n",
    "        logger.info(\"YouTube API service object created.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building YouTube service: {e}\")\n",
    "        youtube_service = None\n",
    "else:\n",
    "    logger.warning(\"YouTube API Key not loaded, skipping YouTube service initialization.\")\n",
    "    youtube_service = None\n",
    "\n",
    "# --- Initialize Transformer Sentiment Pipeline ---\n",
    "logger.info(\"Initializing Transformer Sentiment Pipeline for YouTube...\")\n",
    "device_num = 0 if torch.cuda.is_available() else -1\n",
    "device_name = 'GPU' if device_num == 0 else 'CPU'\n",
    "logger.info(f\"Attempting to load pipeline on device: {device_name}\")\n",
    "\n",
    "try:\n",
    "    # Using RoBERTa model - good for comments/reviews\n",
    "    model_name_sent = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    logger.info(f\"Loading sentiment pipeline: {model_name_sent}...\")\n",
    "    sentiment_pipeline_yt = pipeline(\"sentiment-analysis\", model=model_name_sent, device=device_num)\n",
    "    logger.info(\"Sentiment analysis pipeline loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"ERROR loading sentiment pipeline: {e}. Sentiment analysis will fail.\", exc_info=True)\n",
    "    sentiment_pipeline_yt = None\n",
    "\n",
    "# --- Removed Product Data Loading ---\n",
    "\n",
    "print(\"--- Configuration, Globals, and Initializations Complete (YouTube Focus) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1744700667259,
     "user": {
      "displayName": "Syam chand Yannakula",
      "userId": "16071886527845320126"
     },
     "user_tz": -330
    },
    "id": "yWb2IUt1LwTM",
    "outputId": "3af7acc2-93e8-4fb3-db05-8314b4a12247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- YouTube Helper & NLP Utility Functions Defined ---\n",
      "--- NLP Utility & YT Helper Functions Defined (with Example Comments) ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: NLP Utility & YouTube Helper Functions\n",
    "\n",
    "# --- Library Imports needed ---\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "import html\n",
    "import torch # For type hints if needed, and checking GPU\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from googleapiclient.errors import HttpError\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "from youtube_transcript_api._errors import RequestBlocked\n",
    "\n",
    "# Ensure nltk and stop_words_nltk (global) are available from Cell 2\n",
    "# Ensure sentence_tokenizer (global) is available from Cell 3\n",
    "# Ensure vader_analyzer (global) is available from Cell 3 (if needed)\n",
    "# Ensure sentiment_pipeline_yt (global) is available from Cell 3\n",
    "# Ensure youtube_service (global) is available from Cell 3\n",
    "# Ensure YOUTUBE_CATEGORY_MAP (global) is available from Cell 3\n",
    "\n",
    "\n",
    "# --- Text Preprocessing ---\n",
    "def preprocess_text(text, remove_stopwords=False):\n",
    "    \"\"\"Cleans text: lowercase, removes URL, punctuation, numbers, optional stopwords.\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = text.split()\n",
    "    if remove_stopwords:\n",
    "        custom_stopwords = {'video', 'channel', 'youtube', 'comment', 'watch', 'thanks', 'subscriber', 'like'} # YT specific\n",
    "        stopwords_to_remove = stop_words_nltk.union(custom_stopwords)\n",
    "        words = [word for word in words if word not in stopwords_to_remove and len(word) > 2]\n",
    "    text = \" \".join(words)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- Sentiment Wrapper (using YT pipeline) ---\n",
    "# In Cell 4\n",
    "\n",
    "def get_sentiment_label(text):\n",
    "    \"\"\"Gets Positive/Negative/Neutral label using the SHARED pipeline.\"\"\"\n",
    "    global sentiment_pipeline_yt\n",
    "    if not sentiment_pipeline_yt or not isinstance(text, str) or not text.strip():\n",
    "        logger.warning(\"Sentiment pipeline unavailable or invalid input.\")\n",
    "        return \"Neutral\"\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "             # Rely explicitly on pipeline's truncation, ensure input isn't excessively long initially\n",
    "             # Reduce slice slightly just in case of tokenization expansion\n",
    "             result = sentiment_pipeline_yt(text[:500], truncation=True, max_length=512)[0]\n",
    "        label_map = {\"LABEL_0\": \"Negative\", \"LABEL_1\": \"Neutral\", \"LABEL_2\": \"Positive\"}\n",
    "        return label_map.get(result['label'], \"Neutral\")\n",
    "    # Catch the specific size mismatch error if possible (might be RuntimeError or ValueError depending on backend)\n",
    "    except (RuntimeError, ValueError) as e:\n",
    "         if \"size\" in str(e).lower() and \"match\" in str(e).lower():\n",
    "              logger.error(f\"Tensor size mismatch error during sentiment analysis: {e}. Input text likely too long/complex after tokenization. Text: '{text[:100]}...'\")\n",
    "              return \"Error (Input Length)\" # Specific error label\n",
    "         else:\n",
    "              logger.error(f\"Error during shared sentiment analysis (Non-size): {e}\")\n",
    "              return \"Error (Analysis)\" # Generic analysis error\n",
    "    except Exception as e: # Catch other unexpected errors\n",
    "        logger.error(f\"Unexpected error during shared sentiment analysis: {e}\", exc_info=True)\n",
    "        return \"Error (Analysis)\"\n",
    "# --- Summarize Description (using explicit sentence tokenizer) ---\n",
    "def summarize_description(description, num_sentences=5):\n",
    "    \"\"\"Creates summary using explicitly loaded sentence tokenizer.\"\"\"\n",
    "    global sentence_tokenizer\n",
    "    if not sentence_tokenizer:\n",
    "        return \"Could not generate summary (Tokenizer unavailable).\"\n",
    "    if not description or not isinstance(description, str):\n",
    "        return \"No description available to summarize.\"\n",
    "    try:\n",
    "        sentences = sentence_tokenizer.tokenize(description)\n",
    "        summary_sentences = sentences[:num_sentences]\n",
    "        summary = \" \".join(summary_sentences).strip()\n",
    "        if len(sentences) > num_sentences and summary: summary += \"...\"\n",
    "        if not summary: return \"Description too short to summarize.\"\n",
    "        elif len(summary) < 30: return description\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during description summarization: {e}\")\n",
    "        return \"Could not generate summary from description.\"\n",
    "\n",
    "# --- YouTube Category Name Lookup ---\n",
    "def get_category_name(category_id):\n",
    "    \"\"\"Looks up the category name from the ID using global map.\"\"\"\n",
    "    global YOUTUBE_CATEGORY_MAP\n",
    "    if not category_id: return \"Unknown\"\n",
    "    return YOUTUBE_CATEGORY_MAP.get(str(category_id), \"Unknown/Other\")\n",
    "\n",
    "# --- Extract Video ID ---\n",
    "def extract_video_id(url):\n",
    "    \"\"\"Extracts YouTube video ID from various URL formats.\"\"\"\n",
    "    if not isinstance(url, str): return None\n",
    "    patterns = [\n",
    "        r'(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/watch\\?v=([a-zA-Z0-9_-]{11})',\n",
    "        r'(?:https?:\\/\\/)?(?:www\\.)?youtu\\.be\\/([a-zA-Z0-9_-]{11})',\n",
    "        r'(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/embed\\/([a-zA-Z0-9_-]{11})',\n",
    "        r'(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/shorts\\/([a-zA-Z0-9_-]{11})',\n",
    "        r'^([a-zA-Z0-9_-]{11})$' ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, url)\n",
    "        if match: return match.group(1)\n",
    "    return None\n",
    "\n",
    "# --- Search YouTube Videos ---\n",
    "def search_youtube_video(query, max_results=1):\n",
    "    \"\"\"Searches YouTube using the global API service.\"\"\"\n",
    "    global youtube_service\n",
    "    if not youtube_service: logger.error(\"YT service not available for search.\"); return None\n",
    "    if not query or not isinstance(query, str): return []\n",
    "    results = []\n",
    "    try:\n",
    "        request = youtube_service.search().list(part=\"id,snippet\", q=query, type=\"video\", maxResults=max_results, relevanceLanguage=\"en\")\n",
    "        response = request.execute()\n",
    "        for item in response.get('items', []):\n",
    "            if item.get('id', {}).get('kind') == 'youtube#video':\n",
    "                video_id = item.get('id', {}).get('videoId')\n",
    "                if video_id:\n",
    "                    results.append({\n",
    "                        'video_id': video_id,\n",
    "                        'title': item.get('snippet', {}).get('title', 'N/A'),\n",
    "                        'channel_title': item.get('snippet', {}).get('channelTitle', 'N/A') })\n",
    "    except Exception as e: logger.error(f\"YouTube search failed for '{query}': {e}\")\n",
    "    return results\n",
    "\n",
    "# --- Get Video Details (incl. Category) ---\n",
    "def get_video_details(video_id):\n",
    "    \"\"\"Gets video details using the global API service.\"\"\"\n",
    "    global youtube_service\n",
    "    details = {'title': None, 'description': None, 'channel_title': None, 'category_id': None, 'error': None}\n",
    "    if not video_id: details['error'] = \"No Video ID\"; return details\n",
    "    if not youtube_service: details['error'] = \"API service unavailable\"; return details\n",
    "    try:\n",
    "        request = youtube_service.videos().list(part=\"snippet\", id=str(video_id))\n",
    "        response = request.execute()\n",
    "        if response.get('items'):\n",
    "            snippet = response['items'][0].get('snippet', {})\n",
    "            details.update({\n",
    "                'title': snippet.get('title'), 'description': snippet.get('description'),\n",
    "                'channel_title': snippet.get('channelTitle'), 'category_id': snippet.get('categoryId') })\n",
    "        else: details['error'] = \"Video not found\"\n",
    "    except Exception as e: details['error'] = f\"API Error fetching details: {type(e).__name__}\"; logger.error(f\"YT Details Error {video_id}: {e}\")\n",
    "    return details\n",
    "\n",
    "# --- Get YouTube Comments ---\n",
    "def get_youtube_comments(video_id, max_comments=75):\n",
    "    \"\"\"Gets comments using the global API service.\"\"\"\n",
    "    global youtube_service\n",
    "    comments, error_msg = [], None\n",
    "    if not video_id: return [], \"No Video ID\"\n",
    "    if not youtube_service: return [], \"API service unavailable\"\n",
    "    try:\n",
    "        request = youtube_service.commentThreads().list(part=\"snippet\", videoId=str(video_id), textFormat=\"plainText\", order=\"relevance\", maxResults=min(max_comments, 100))\n",
    "        response = request.execute()\n",
    "        for item in response.get('items', []):\n",
    "            try: comments.append(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "            except KeyError: pass\n",
    "        if not comments and 'items' not in response: error_msg = \"No comments found\"\n",
    "    except HttpError as e:\n",
    "        err_content = e.content.decode('utf-8','ignore').lower()\n",
    "        error_msg = \"Comments disabled\" if e.resp.status == 403 and \"disabled comments\" in err_content else f\"API Error {e.resp.status}\"\n",
    "        logger.error(f\"YT Comment fetch {error_msg} for {video_id}\")\n",
    "    except Exception as e: error_msg = f\"Comment Error: {type(e).__name__}\"; logger.error(f\"{error_msg} for {video_id}\")\n",
    "    return comments, error_msg\n",
    "\n",
    "# --- Get YouTube Transcript (Placeholder/Unreliable) ---\n",
    "def get_youtube_transcript(video_id):\n",
    "    \"\"\"Attempts to get transcript, handles known errors.\"\"\"\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi.get_transcript(str(video_id), languages=['en', 'en-US'])\n",
    "        return \" \".join([item['text'] for item in transcript_list]), None\n",
    "    except RequestBlocked: return None, \"Request blocked by YouTube (Colab issue)\"\n",
    "    except TranscriptsDisabled: return None, \"Transcripts disabled\"\n",
    "    except NoTranscriptFound: return None, \"No English transcript found\"\n",
    "    except Exception as e: logger.warning(f\"YT Transcript Error {video_id}: {e}\"); return None, f\"Transcript Error: {type(e).__name__}\"\n",
    "\n",
    "# --- Keyword Extraction ---\n",
    "def extract_youtube_keywords(title, description, comments, num_keywords=7):\n",
    "    \"\"\"Extracts keywords using TF-IDF.\"\"\"\n",
    "    text_for_keywords = []\n",
    "    if title: text_for_keywords.append(title)\n",
    "    if description: text_for_keywords.append(description)\n",
    "    if comments: text_for_keywords.extend(comments)\n",
    "    if not text_for_keywords: return []\n",
    "    processed_texts = [preprocess_text(text, remove_stopwords=True) for text in text_for_keywords if isinstance(text, str) and text]\n",
    "    if not processed_texts: return []\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=3000, stop_words='english', ngram_range=(1,2))\n",
    "        tfidf_matrix = vectorizer.fit_transform(processed_texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        sum_tfidf = tfidf_matrix.sum(axis=0)\n",
    "        scores = [(feature_names[col], sum_tfidf[0, col]) for col in range(sum_tfidf.shape[1])]\n",
    "        sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        return [keyword for keyword, score in sorted_scores[:num_keywords]]\n",
    "    except Exception as e: logger.error(f\"TF-IDF Error: {e}\"); return []\n",
    "\n",
    "print(\"--- YouTube Helper & NLP Utility Functions Defined ---\")\n",
    "def extract_example_comments(comments, num_examples=2):\n",
    "    \"\"\"\n",
    "    Extracts example positive/negative comments using VADER scores.\n",
    "    Args: comments (list), num_examples (int)\n",
    "    Returns: tuple (list_of_pos_examples, list_of_neg_examples)\n",
    "    \"\"\"\n",
    "    global vader_analyzer # Uses global VADER analyzer from Cell 3\n",
    "    pos_examples = []\n",
    "    neg_examples = []\n",
    "    if not comments or not vader_analyzer:\n",
    "        if not vader_analyzer: logger.error(\"VADER analyzer missing for example comments.\")\n",
    "        return pos_examples, neg_examples\n",
    "    comments_with_scores = []\n",
    "    for comment in comments:\n",
    "        if isinstance(comment, str) and 5 < len(comment.split()) < 150:\n",
    "             try:\n",
    "                 score = vader_analyzer.polarity_scores(comment)['compound']\n",
    "                 comments_with_scores.append({'text': comment, 'score': score})\n",
    "             except Exception as e: logger.warning(f\"VADER scoring failed for comment: {comment[:50]}... Error: {e}\")\n",
    "    comments_with_scores.sort(key=lambda x: x['score'], reverse=True)\n",
    "    for data in comments_with_scores:\n",
    "        if len(pos_examples) < num_examples and data['score'] >= 0.5: pos_examples.append(data['text'])\n",
    "        if len(pos_examples) >= num_examples: break\n",
    "    for data in reversed(comments_with_scores):\n",
    "        if len(neg_examples) < num_examples and data['score'] <= -0.4: neg_examples.append(data['text'])\n",
    "        if len(neg_examples) >= num_examples: break\n",
    "    logger.info(f\"Extracted {len(pos_examples)} pos and {len(neg_examples)} neg comment examples.\")\n",
    "    return pos_examples, neg_examples\n",
    "\n",
    "print(\"--- NLP Utility & YT Helper Functions Defined (with Example Comments) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1744700667277,
     "user": {
      "displayName": "Syam chand Yannakula",
      "userId": "16071886527845320126"
     },
     "user_tz": -330
    },
    "id": "b62w73OhLwVz",
    "outputId": "006a159b-52d6-4e92-b455-b1bfc8bffc27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- YouTube Orchestrator & Formatter Defined (with Snippet, Summary & Examples) ---\n"
     ]
    }
   ],
   "source": [
    "'''# Cell 5: YouTube Orchestrator & Verdict Formatter\n",
    "\n",
    "import html # Ensure imported\n",
    "import logging # Ensure imported\n",
    "# Assumes functions from Cell 4 are defined\n",
    "# Assumes sentiment_pipeline_yt (global) is loaded from Cell 3\n",
    "\n",
    "# --- Overall Verdict Calculator (Needed for YT comments too) ---\n",
    "def calculate_overall_verdict(sentiments):\n",
    "    \"\"\"Calculates an overall verdict based on sentiment labels.\"\"\"\n",
    "    if not sentiments: return \"Not Enough Data\"\n",
    "    total = len(sentiments)\n",
    "    pos_count = sentiments.count(\"Positive\")\n",
    "    neg_count = sentiments.count(\"Negative\")\n",
    "    pos_ratio = pos_count / total\n",
    "    neg_ratio = neg_count / total\n",
    "    if pos_ratio >= 0.65 and neg_ratio < 0.15: return \"Overwhelmingly Positive 👍\"\n",
    "    elif pos_ratio > neg_ratio + 0.15 and pos_ratio >= 0.40: return \"Generally Positive 🙂\"\n",
    "    elif neg_ratio >= 0.65 and pos_ratio < 0.15: return \"Overwhelmingly Negative 👎\"\n",
    "    elif neg_ratio > pos_ratio + 0.15 and neg_ratio >= 0.40: return \"Generally Negative 🙁\"\n",
    "    elif abs(pos_ratio - neg_ratio) < 0.20 and (pos_count + neg_count) / total > 0.5: return \"Mixed Reviews 🤔\"\n",
    "    else: return \"Neutral / Balanced 😐\"\n",
    "\n",
    "# --- YouTube Verdict Formatter ---\n",
    "def format_youtube_verdict(details, comments_count, sentiment_results, keywords, category_name, summary, transcript_error=None):\n",
    "    \"\"\"Formats YouTube analysis results into HTML.\"\"\"\n",
    "    verdict = []\n",
    "    verdict.append(f\"🔎 **YouTube Video Analysis**\")\n",
    "    verdict.append(f\"🎬 <b>Title:</b> {html.escape(details.get('title','N/A'))}\")\n",
    "    verdict.append(f\"👤 <b>Channel:</b> {html.escape(details.get('channel_title','N/A'))}\")\n",
    "    verdict.append(f\"📊 <b>Category:</b> {html.escape(category_name)}\")\n",
    "    verdict.append(\"---\")\n",
    "\n",
    "    verdict.append(\"📝 <b>Summary (from Description):</b>\")\n",
    "    verdict.append(f\"<i>{html.escape(summary)}</i>\")\n",
    "    verdict.append(\"---\")\n",
    "\n",
    "    verdict.append(\"💬 **Comment Analysis:**\")\n",
    "    verdict.append(f\"  Comments Found: {comments_count}\")\n",
    "    if sentiment_results.get('error'):\n",
    "        verdict.append(f\"  Sentiment: <i>Error - {html.escape(sentiment_results['error'])}</i>\")\n",
    "    elif comments_count > 0:\n",
    "        label = sentiment_results.get('label', 'Neutral')\n",
    "        pos_count = sentiment_results.get('pos', 0)\n",
    "        neg_count = sentiment_results.get('neg', 0)\n",
    "        neu_count = sentiment_results.get('neu', 0)\n",
    "        emoji = \"😊\" if \"Positive\" in label else (\"😠\" if \"Negative\" in label else \"😐\") # Check label substring\n",
    "        verdict.append(f\"  Overall Sentiment (Model): {emoji} <b>{label}</b>\")\n",
    "        verdict.append(f\"     <i>(Pos: {pos_count}, Neg: {neg_count}, Neu: {neu_count})</i>\")\n",
    "    else:\n",
    "         verdict.append(f\"  Sentiment: <i>No comments found or analysis failed</i>\")\n",
    "    verdict.append(\"---\")\n",
    "\n",
    "    verdict.append(\"🔑 **Keywords:**\")\n",
    "    if keywords:\n",
    "        verdict.append(f\"  <code>{html.escape(', '.join(keywords))}</code>\")\n",
    "    else:\n",
    "         verdict.append(\"  <i>Could not extract keywords.</i>\")\n",
    "\n",
    "    # Add transcript status if error occurred\n",
    "    if transcript_error:\n",
    "         verdict.append(\"---\")\n",
    "         verdict.append(f\"<i>Transcript Status: {html.escape(transcript_error)}</i>\")\n",
    "\n",
    "    return \"\\n\".join(verdict)\n",
    "\n",
    "# --- YouTube Orchestrator ---\n",
    "def analyze_youtube_video_orchestrator(video_id):\n",
    "    \"\"\"Orchestrates YouTube analysis.\"\"\"\n",
    "    global sentiment_pipeline_yt # Ensure access to the loaded pipeline\n",
    "\n",
    "    if not video_id: return \"<b>Error:</b> No video ID provided.\"\n",
    "    logger.info(f\"Orchestrating YouTube analysis for ID: {video_id}\")\n",
    "\n",
    "    # 1. Get Details\n",
    "    video_details = get_video_details(video_id)\n",
    "    if video_details.get('error') and not video_details.get('title'):\n",
    "        return f\"<b>Error:</b> Could not fetch video details. {html.escape(video_details.get('error','Unknown Error'))}\"\n",
    "\n",
    "    # 2. Get Comments\n",
    "    comments, comments_error = get_youtube_comments(video_id)\n",
    "\n",
    "    # 3. Get Transcript Status\n",
    "    _, transcript_error = get_youtube_transcript(video_id)\n",
    "\n",
    "    # 4. Analyze Comment Sentiment\n",
    "    sentiment_results_yt = {'label': 'Neutral', 'pos': 0, 'neg': 0, 'neu': 0, 'error': comments_error}\n",
    "    comment_sentiments_list = []\n",
    "    if comments:\n",
    "        if sentiment_pipeline_yt:\n",
    "             comment_sentiments_list = [get_sentiment_label(c) for c in comments]\n",
    "             sentiment_results_yt['pos'] = comment_sentiments_list.count(\"Positive\")\n",
    "             sentiment_results_yt['neg'] = comment_sentiments_list.count(\"Negative\")\n",
    "             sentiment_results_yt['neu'] = comment_sentiments_list.count(\"Neutral\")\n",
    "             sentiment_results_yt['label'] = calculate_overall_verdict(comment_sentiments_list)\n",
    "             sentiment_results_yt['error'] = None # Clear fetch error if analysis ran\n",
    "             logger.info(f\"YT sentiment analysis complete for {len(comments)} comments.\")\n",
    "        else:\n",
    "            sentiment_results_yt['error'] = \"Sentiment Analyzer not available.\"\n",
    "            logger.error(sentiment_results_yt['error'])\n",
    "    elif comments_error: # If comments failed to fetch, report that as the sentiment error\n",
    "         sentiment_results_yt['error'] = comments_error\n",
    "\n",
    "    # 5. Extract Keywords\n",
    "    keywords = extract_youtube_keywords(video_details.get('title'), video_details.get('description'), comments)\n",
    "\n",
    "    # 6. Get Category Name\n",
    "    category_name = get_category_name(video_details.get('category_id'))\n",
    "\n",
    "    # 7. Summarize Description\n",
    "    summary = summarize_description(video_details.get('description'))\n",
    "\n",
    "    # 8. Generate Verdict\n",
    "    verdict = format_youtube_verdict(\n",
    "        video_details,\n",
    "        len(comments) if comments else 0, # Pass comment count\n",
    "        sentiment_results_yt,\n",
    "        keywords,\n",
    "        category_name,\n",
    "        summary,\n",
    "        transcript_error # Pass transcript status\n",
    "    )\n",
    "\n",
    "    logger.info(f\"YouTube analysis orchestration complete for ID: {video_id}\")\n",
    "    return verdict\n",
    "\n",
    "\n",
    "print(\"--- YouTube Orchestrator & Formatter Defined ---\")'''\n",
    "'''# Cell 5: YouTube Orchestrator & Verdict Formatter (Updated)\n",
    "\n",
    "import html # Ensure imported\n",
    "import logging # Ensure imported\n",
    "# Assumes functions from Cell 4 are defined\n",
    "# Assumes sentiment_pipeline_yt (global) is loaded from Cell 3\n",
    "\n",
    "# --- Overall Verdict Calculator (Keep as before) ---\n",
    "def calculate_overall_verdict(sentiments):\n",
    "    # ... (definition remains the same) ...\n",
    "    if not sentiments: return \"Not Enough Data\"\n",
    "    total = len(sentiments); pos_count = sentiments.count(\"Positive\"); neg_count = sentiments.count(\"Negative\")\n",
    "    pos_ratio = pos_count / total; neg_ratio = neg_count / total\n",
    "    if pos_ratio >= 0.65 and neg_ratio < 0.15: return \"Overwhelmingly Positive 👍\"\n",
    "    elif pos_ratio > neg_ratio + 0.15 and pos_ratio >= 0.40: return \"Generally Positive 🙂\"\n",
    "    elif neg_ratio >= 0.65 and pos_ratio < 0.15: return \"Overwhelmingly Negative 👎\"\n",
    "    elif neg_ratio > pos_ratio + 0.15 and neg_ratio >= 0.40: return \"Generally Negative 🙁\"\n",
    "    elif abs(pos_ratio - neg_ratio) < 0.20 and (pos_count + neg_count) / total > 0.5: return \"Mixed Reviews 🤔\"\n",
    "    else: return \"Neutral / Balanced 😐\"\n",
    "\n",
    "# --- YouTube Verdict Formatter (Updated) ---\n",
    "def format_youtube_verdict(details, comments_count, sentiment_results, keywords, category_name, single_line_summary, transcript_error=None, pos_examples=None, neg_examples=None): # Added examples\n",
    "    \"\"\"Formats YouTube analysis results into HTML, including example comments.\"\"\"\n",
    "    verdict = []\n",
    "    verdict.append(f\"🔎 **YouTube Video Analysis**\")\n",
    "    verdict.append(f\"🎬 <b>Title:</b> {html.escape(details.get('title','N/A'))}\")\n",
    "    verdict.append(f\"👤 <b>Channel:</b> {html.escape(details.get('channel_title','N/A'))}\")\n",
    "    verdict.append(f\"📊 <b>Category:</b> {html.escape(category_name)}\")\n",
    "    verdict.append(\"---\")\n",
    "\n",
    "    verdict.append(\"📝 <b>Content Snippet (from Description):</b>\") # Changed Title\n",
    "    verdict.append(f\"<i>{html.escape(single_line_summary)}</i>\") # Display single line\n",
    "    verdict.append(\"---\")\n",
    "\n",
    "    verdict.append(\"💬 **Comment Analysis:**\")\n",
    "    verdict.append(f\"  Comments Found: {comments_count}\")\n",
    "    if sentiment_results.get('error'):\n",
    "        verdict.append(f\"  Sentiment: <i>Error - {html.escape(sentiment_results['error'])}</i>\")\n",
    "    elif comments_count > 0:\n",
    "        label = sentiment_results.get('label', 'Neutral')\n",
    "        pos_count = sentiment_results.get('pos', 0)\n",
    "        neg_count = sentiment_results.get('neg', 0)\n",
    "        neu_count = sentiment_results.get('neu', 0)\n",
    "        emoji = \"😊\" if \"Positive\" in label else (\"😠\" if \"Negative\" in label else \"😐\")\n",
    "        verdict.append(f\"  Overall Sentiment (Model): {emoji} <b>{label}</b>\")\n",
    "        verdict.append(f\"     <i>(Pos: {pos_count}, Neg: {neg_count}, Neu: {neu_count})</i>\")\n",
    "    else:\n",
    "         verdict.append(f\"  Sentiment: <i>No comments found or analysis failed</i>\")\n",
    "\n",
    "    # --- Add Example Comments Section ---\n",
    "    if pos_examples:\n",
    "        verdict.append(\"\\n👍 <b>Example Positive Comments:</b>\")\n",
    "        for i, comment in enumerate(pos_examples):\n",
    "            verdict.append(f\"  {i+1}. <i>{html.escape(comment[:200])}...</i>\") # Limit length\n",
    "    if neg_examples:\n",
    "        verdict.append(\"\\n👎 <b>Example Negative Comments:</b>\")\n",
    "        for i, comment in enumerate(neg_examples):\n",
    "            verdict.append(f\"  {i+1}. <i>{html.escape(comment[:200])}...</i>\") # Limit length\n",
    "    # --- End Example Comments Section ---\n",
    "\n",
    "    verdict.append(\"---\")\n",
    "\n",
    "    verdict.append(\"🔑 **Keywords:**\")\n",
    "    if keywords:\n",
    "        verdict.append(f\"  <code>{html.escape(', '.join(keywords))}</code>\")\n",
    "    else:\n",
    "         verdict.append(\"  <i>Could not extract keywords.</i>\")\n",
    "\n",
    "    if transcript_error:\n",
    "         verdict.append(\"---\")\n",
    "         verdict.append(f\"<i>Transcript Status: {html.escape(transcript_error)}</i>\") # Using italics\n",
    "\n",
    "    return \"\\n\".join(verdict)\n",
    "\n",
    "# --- YouTube Orchestrator (Updated) ---\n",
    "def analyze_youtube_video_orchestrator(video_id):\n",
    "    \"\"\"Orchestrates YouTube analysis, including example comments and single line summary.\"\"\"\n",
    "    global sentiment_pipeline_yt # Ensure access to the loaded pipeline\n",
    "\n",
    "    if not video_id: return \"<b>Error:</b> No video ID provided.\"\n",
    "    logger.info(f\"Orchestrating YouTube analysis for ID: {video_id}\")\n",
    "\n",
    "    # 1. Get Details\n",
    "    video_details = get_video_details(video_id) # Uses global service\n",
    "    if video_details.get('error') and not video_details.get('title'):\n",
    "        return f\"<b>Error:</b> Could not fetch video details. {html.escape(video_details.get('error','Unknown Error'))}\"\n",
    "\n",
    "    # 2. Get Comments\n",
    "    comments, comments_error = get_youtube_comments(video_id) # Uses global service\n",
    "\n",
    "    # 3. Get Transcript Status\n",
    "    _, transcript_error = get_youtube_transcript(video_id)\n",
    "\n",
    "    # 4. Analyze Comment Sentiment\n",
    "    sentiment_results_yt = {'label': 'Neutral', 'pos': 0, 'neg': 0, 'neu': 0, 'error': comments_error}\n",
    "    comment_sentiments_list = []\n",
    "    if comments: # Only proceed if comments were found\n",
    "        if sentiment_pipeline_yt:\n",
    "             comment_sentiments_list = [get_sentiment_label(c) for c in comments]\n",
    "             sentiment_results_yt['pos'] = comment_sentiments_list.count(\"Positive\")\n",
    "             sentiment_results_yt['neg'] = comment_sentiments_list.count(\"Negative\")\n",
    "             sentiment_results_yt['neu'] = comment_sentiments_list.count(\"Neutral\")\n",
    "             sentiment_results_yt['label'] = calculate_overall_verdict(comment_sentiments_list)\n",
    "             sentiment_results_yt['error'] = None\n",
    "             logger.info(f\"YT sentiment analysis complete for {len(comments)} comments.\")\n",
    "        else:\n",
    "            sentiment_results_yt['error'] = \"Sentiment Analyzer not available.\"\n",
    "            logger.error(sentiment_results_yt['error'])\n",
    "    # Keep comments_error if fetching failed initially\n",
    "    elif comments_error:\n",
    "         sentiment_results_yt['error'] = comments_error\n",
    "\n",
    "    # 5. Extract Example Comments (Use raw comments before potential filtering)\n",
    "    pos_example_comments, neg_example_comments = extract_example_comments(comments, num_examples=2)\n",
    "\n",
    "    # 6. Extract Keywords\n",
    "    keywords = extract_youtube_keywords(video_details.get('title'), video_details.get('description'), comments)\n",
    "\n",
    "    # 7. Get Category Name\n",
    "    category_name = get_category_name(video_details.get('category_id'))\n",
    "\n",
    "    # 8. Get Single Line Summary\n",
    "    single_line_summary = summarize_description(video_details.get('description'), num_sentences=1) # Request only 1 sentence\n",
    "\n",
    "    # 9. Generate Verdict\n",
    "    verdict = format_youtube_verdict(\n",
    "        video_details,\n",
    "        len(comments) if comments else 0,\n",
    "        sentiment_results_yt,\n",
    "        keywords,\n",
    "        category_name,\n",
    "        single_line_summary,\n",
    "        transcript_error,\n",
    "        pos_examples=pos_example_comments, # Pass examples\n",
    "        neg_examples=neg_example_comments  # Pass examples\n",
    "    )\n",
    "\n",
    "    logger.info(f\"YouTube analysis orchestration complete for ID: {video_id}\")\n",
    "    return verdict\n",
    "\n",
    "print(\"--- YouTube Orchestrator & Formatter Defined (with Examples & Single Line Summary) ---\")'''\n",
    "# Cell 5: YouTube Orchestrator & Verdict Formatter (Restoring Multi-Sentence Summary)\n",
    "\n",
    "import html\n",
    "import logging\n",
    "# Assumes functions from Cell 4 are defined\n",
    "# Assumes sentiment_pipeline_yt is loaded globally\n",
    "\n",
    "# --- Overall Verdict Calculator (Keep as before) ---\n",
    "def calculate_overall_verdict(sentiments):\n",
    "    # ... (definition remains the same) ...\n",
    "    if not sentiments: return \"Not Enough Data\"\n",
    "    total = len(sentiments); pos_count = sentiments.count(\"Positive\"); neg_count = sentiments.count(\"Negative\")\n",
    "    pos_ratio = pos_count / total; neg_ratio = neg_count / total\n",
    "    if pos_ratio >= 0.65 and neg_ratio < 0.15: return \"Overwhelmingly Positive 👍\"\n",
    "    elif pos_ratio > neg_ratio + 0.15 and pos_ratio >= 0.40: return \"Generally Positive 🙂\"\n",
    "    elif neg_ratio >= 0.65 and pos_ratio < 0.15: return \"Overwhelmingly Negative 👎\"\n",
    "    elif neg_ratio > pos_ratio + 0.15 and neg_ratio >= 0.40: return \"Generally Negative 🙁\"\n",
    "    elif abs(pos_ratio - neg_ratio) < 0.20 and (pos_count + neg_count) / total > 0.5: return \"Mixed Reviews 🤔\"\n",
    "    else: return \"Neutral / Balanced 😐\"\n",
    "\n",
    "\n",
    "# --- YouTube Verdict Formatter (Updated) ---\n",
    "def format_youtube_verdict(details, comments_count, sentiment_results, keywords, category_name,\n",
    "                           single_line_snippet, multi_sentence_summary, # Added multi_sentence_summary\n",
    "                           transcript_error=None, pos_examples=None, neg_examples=None):\n",
    "    \"\"\"Formats YouTube analysis results, including snippet, summary, and examples.\"\"\"\n",
    "    verdict = []\n",
    "    verdict.append(f\"🔎 **YouTube Video Analysis**\")\n",
    "    verdict.append(f\"🎬 <b>Title:</b> {html.escape(details.get('title','N/A'))}\")\n",
    "    verdict.append(f\"👤 <b>Channel:</b> {html.escape(details.get('channel_title','N/A'))}\")\n",
    "    verdict.append(f\"📊 <b>Category:</b> {html.escape(category_name)}\")\n",
    "    verdict.append(\"---\")\n",
    "\n",
    "    # Single Line Snippet\n",
    "    verdict.append(\"📝 <b>Content Snippet (from Description):</b>\")\n",
    "    verdict.append(f\"<i>{html.escape(single_line_snippet)}</i>\")\n",
    "    verdict.append(\"---\")\n",
    "\n",
    "    # Multi-Sentence Summary\n",
    "    verdict.append(\"📋 <b>Summary (from Description):</b>\") # New section title\n",
    "    verdict.append(f\"<i>{html.escape(multi_sentence_summary)}</i>\") # Display longer summary\n",
    "    verdict.append(\"---\")\n",
    "\n",
    "\n",
    "    verdict.append(\"💬 **Comment Analysis:**\")\n",
    "    verdict.append(f\"  Comments Found: {comments_count}\")\n",
    "    if sentiment_results.get('error'):\n",
    "        verdict.append(f\"  Sentiment: <i>Error - {html.escape(sentiment_results['error'])}</i>\")\n",
    "    elif comments_count > 0:\n",
    "        label = sentiment_results.get('label', 'Neutral')\n",
    "        pos_count = sentiment_results.get('pos', 0)\n",
    "        neg_count = sentiment_results.get('neg', 0)\n",
    "        neu_count = sentiment_results.get('neu', 0)\n",
    "        emoji = \"😊\" if \"Positive\" in label else (\"😠\" if \"Negative\" in label else \"😐\")\n",
    "        verdict.append(f\"  Overall Sentiment (Model): {emoji} <b>{label}</b>\")\n",
    "        verdict.append(f\"     <i>(Pos: {pos_count}, Neg: {neg_count}, Neu: {neu_count})</i>\")\n",
    "    else:\n",
    "         verdict.append(f\"  Sentiment: <i>No comments found or analysis failed</i>\")\n",
    "\n",
    "    # Example Comments Section (Keep as before)\n",
    "    if pos_examples:\n",
    "        verdict.append(\"\\n👍 <b>Example Positive Comments:</b>\")\n",
    "        for i, comment in enumerate(pos_examples): verdict.append(f\"  {i+1}. <i>{html.escape(comment[:200])}...</i>\")\n",
    "    if neg_examples:\n",
    "        verdict.append(\"\\n👎 <b>Example Negative Comments:</b>\")\n",
    "        for i, comment in enumerate(neg_examples): verdict.append(f\"  {i+1}. <i>{html.escape(comment[:200])}...</i>\")\n",
    "\n",
    "    verdict.append(\"---\")\n",
    "\n",
    "    # Keywords Section (Keep as before)\n",
    "    verdict.append(\"🔑 **Keywords:**\")\n",
    "    if keywords: verdict.append(f\"  <code>{html.escape(', '.join(keywords))}</code>\")\n",
    "    else: verdict.append(\"  <i>Could not extract keywords.</i>\")\n",
    "\n",
    "    # Transcript Status (Keep as before)\n",
    "    if transcript_error:\n",
    "         verdict.append(\"---\")\n",
    "         verdict.append(f\"<i>Transcript Status: {html.escape(transcript_error)}</i>\")\n",
    "\n",
    "    return \"\\n\".join(verdict)\n",
    "\n",
    "\n",
    "# --- YouTube Orchestrator (Updated) ---\n",
    "def analyze_youtube_video_orchestrator(video_id, num_summary_sentences=3): # Added parameter for summary length\n",
    "    \"\"\"Orchestrates YouTube analysis, including snippet, summary and example comments.\"\"\"\n",
    "    global sentiment_pipeline_yt\n",
    "\n",
    "    if not video_id: return \"<b>Error:</b> No video ID provided.\"\n",
    "    logger.info(f\"Orchestrating YouTube analysis for ID: {video_id}\")\n",
    "\n",
    "    # 1. Get Details\n",
    "    video_details = get_video_details(video_id)\n",
    "    if video_details.get('error') and not video_details.get('title'):\n",
    "        return f\"<b>Error:</b> Could not fetch video details. {html.escape(video_details.get('error','Unknown Error'))}\"\n",
    "\n",
    "    # 2. Get Comments\n",
    "    comments, comments_error = get_youtube_comments(video_id)\n",
    "\n",
    "    # 3. Get Transcript Status\n",
    "    _, transcript_error = get_youtube_transcript(video_id)\n",
    "\n",
    "    # 4. Analyze Comment Sentiment\n",
    "    sentiment_results_yt = {'label': 'Neutral', 'pos': 0, 'neg': 0, 'neu': 0, 'error': comments_error}\n",
    "    comment_sentiments_list = []\n",
    "    if comments:\n",
    "        if sentiment_pipeline_yt:\n",
    "             comment_sentiments_list = [get_sentiment_label(c) for c in comments]\n",
    "             sentiment_results_yt['pos'] = comment_sentiments_list.count(\"Positive\")\n",
    "             sentiment_results_yt['neg'] = comment_sentiments_list.count(\"Negative\")\n",
    "             sentiment_results_yt['neu'] = comment_sentiments_list.count(\"Neutral\")\n",
    "             sentiment_results_yt['label'] = calculate_overall_verdict(comment_sentiments_list)\n",
    "             sentiment_results_yt['error'] = None\n",
    "             logger.info(f\"YT sentiment analysis complete for {len(comments)} comments.\")\n",
    "        else:\n",
    "            sentiment_results_yt['error'] = \"Sentiment Analyzer not available.\"\n",
    "            logger.error(sentiment_results_yt['error'])\n",
    "    elif comments_error:\n",
    "         sentiment_results_yt['error'] = comments_error\n",
    "\n",
    "    # 5. Extract Example Comments\n",
    "    pos_example_comments, neg_example_comments = extract_example_comments(comments, num_examples=2)\n",
    "\n",
    "    # 6. Extract Keywords\n",
    "    keywords = extract_youtube_keywords(video_details.get('title'), video_details.get('description'), comments)\n",
    "\n",
    "    # 7. Get Category Name\n",
    "    category_name = get_category_name(video_details.get('category_id'))\n",
    "\n",
    "    # 8. Get Summaries (Snippet AND Longer Summary)\n",
    "    description_text = video_details.get('description')\n",
    "    single_line_snippet = summarize_description(description_text, num_sentences=1)\n",
    "    multi_sentence_summary = summarize_description(description_text, num_sentences=num_summary_sentences) # Use parameter\n",
    "\n",
    "    # 9. Generate Verdict (Pass both summaries)\n",
    "    verdict = format_youtube_verdict(\n",
    "        video_details,\n",
    "        len(comments) if comments else 0,\n",
    "        sentiment_results_yt,\n",
    "        keywords,\n",
    "        category_name,\n",
    "        single_line_snippet,     # Pass snippet\n",
    "        multi_sentence_summary,  # Pass longer summary\n",
    "        transcript_error,\n",
    "        pos_examples=pos_example_comments,\n",
    "        neg_examples=neg_example_comments\n",
    "    )\n",
    "\n",
    "    logger.info(f\"YouTube analysis orchestration complete for ID: {video_id}\")\n",
    "    return verdict\n",
    "\n",
    "\n",
    "print(\"--- YouTube Orchestrator & Formatter Defined (with Snippet, Summary & Examples) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "FjQLZpFnLwYI",
    "outputId": "19bed1c0-eb1b-4465-d913-4056604f865c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bot is starting (YouTube Analyzer ONLY) ---\n",
      "--- Make sure API Key & Token are in Colab Secrets ---\n",
      "--- Ensure Cells 1-6 have run successfully ---\n",
      "Send commands to your bot in Telegram.\n",
      "Interrupt the kernel (Stop button) to stop the bot.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Telegram Bot Code (YouTube Only)\n",
    "\n",
    "# --- Imports ---\n",
    "import logging\n",
    "import asyncio\n",
    "import html\n",
    "# Ensure nest_asyncio was imported and applied in Cell 2\n",
    "from telegram import Update, constants\n",
    "from telegram.ext import (\n",
    "    Application,\n",
    "    CommandHandler,\n",
    "    MessageHandler,\n",
    "    filters,\n",
    "    ContextTypes,\n",
    "    ApplicationBuilder\n",
    ")\n",
    "from telegram.error import BadRequest\n",
    "# --- Assumes previous cells (1-5) defining functions/globals ran ---\n",
    "# --- Includes analyze_youtube_video_orchestrator from Cell 5 ---\n",
    "\n",
    "# --- Bot Command Handlers ---\n",
    "\n",
    "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    user_name = update.effective_user.first_name\n",
    "    cmd1_escaped = html.escape(\"/youtube <YouTube URL or Video Title>\")\n",
    "    await update.message.reply_html(\n",
    "        f\"Hi {user_name}!\\n\\n\"\n",
    "        f\"I can analyze YouTube videos.\\n\\n\"\n",
    "        f\"Use:\\n\"\n",
    "        f\"<code>{cmd1_escaped}</code>\\n\\n\"\n",
    "        f\"Use /help for more info.\"\n",
    "    )\n",
    "\n",
    "async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    cmd1_escaped = html.escape(\"/youtube <YouTube URL or Video Title>\")\n",
    "    await update.message.reply_html(\n",
    "        f\"<b>YouTube Video Analyzer Bot</b>\\n\\n\"\n",
    "        f\"Send the command:\\n\"\n",
    "        f\"<code>{cmd1_escaped}</code>\\n\\n\"\n",
    "        f\"I will analyze the video's details (Title, Channel, Category), summarize its description, analyze comment sentiment, and extract keywords.\\n\\n\"\n",
    "        f\"<i>Note: Transcript fetching from Colab is often blocked by YouTube.</i>\"\n",
    "    )\n",
    "\n",
    "\n",
    "async def handle_youtube(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    \"\"\"Handles the /youtube command, processing URL or title.\"\"\"\n",
    "    user_input = \" \".join(context.args) if context.args else None\n",
    "    if not user_input:\n",
    "        await update.message.reply_text(\"Please provide a YouTube URL or Video Title after /youtube.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Received /youtube command with input: '{user_input}'\")\n",
    "    await update.message.reply_html(\"<i>Okay, looking for that YouTube video...</i>\")\n",
    "    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=constants.ChatAction.TYPING)\n",
    "\n",
    "    video_id = None\n",
    "    video_title_search = None\n",
    "\n",
    "    potential_id = extract_video_id(user_input)\n",
    "    if potential_id:\n",
    "        video_id = potential_id\n",
    "        await update.message.reply_html(f\"Analyzing YouTube video with ID: <code>{video_id}</code>...\")\n",
    "    else:\n",
    "        if not youtube_service:\n",
    "             await update.message.reply_html(\"<b>Error:</b> YouTube search unavailable (API Key/Service missing?). Please use a direct URL.\")\n",
    "             return\n",
    "        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=constants.ChatAction.TYPING)\n",
    "        search_results = await asyncio.to_thread(search_youtube_video, user_input, max_results=1)\n",
    "        if search_results is None: await update.message.reply_html(\"<b>Error:</b> YouTube search failed (API issue?).\"); return\n",
    "        elif not search_results: await update.message.reply_html(f\"<b>Sorry:</b> Couldn't find YouTube videos matching '<i>{html.escape(user_input)}</i>'.\"); return\n",
    "        else:\n",
    "            top_result = search_results[0]\n",
    "            video_id = top_result['video_id']\n",
    "            video_title_search = top_result['title']\n",
    "            channel = top_result['channel_title']\n",
    "            await update.message.reply_html(f\"Found: <b>{html.escape(video_title_search)}</b> by <i>{html.escape(channel)}</i>.\\nAnalyzing...\")\n",
    "\n",
    "    if video_id:\n",
    "        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=constants.ChatAction.TYPING)\n",
    "        try:\n",
    "            if not youtube_service: # Check again before analysis call\n",
    "                 raise ConnectionError(\"YouTube API Service not available for analysis.\")\n",
    "\n",
    "            # Run the YouTube orchestrator (Cell 5) asynchronously\n",
    "            verdict = await asyncio.to_thread(analyze_youtube_video_orchestrator, video_id)\n",
    "            await update.message.reply_html(verdict) # Formatter (Cell 5) produces HTML\n",
    "\n",
    "        except ConnectionError as ce:\n",
    "             logger.error(f\"YouTube analysis connection error for ID {video_id}: {ce}\")\n",
    "             await update.message.reply_html(f\"<b>Error:</b> Cannot perform YouTube analysis. {html.escape(str(ce))}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during YouTube analysis orchestration for ID {video_id}: {e}\", exc_info=True)\n",
    "            await update.message.reply_html(f\"<b>Error:</b> An unexpected error occurred during YouTube analysis: <i>{html.escape(type(e).__name__)}</i>\")\n",
    "    else:\n",
    "        await update.message.reply_html(\"<b>Error:</b> Could not determine the YouTube video to analyze.\")\n",
    "\n",
    "\n",
    "async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    \"\"\"Log Errors, potentially ignore HTML parsing errors.\"\"\"\n",
    "    if isinstance(context.error, BadRequest) and \"message is not modified\" in str(context.error).lower():\n",
    "        logger.warning(f\"Ignoring 'Message is not modified' error: {context.error}\")\n",
    "        return # Don't log this common benign error\n",
    "    if isinstance(context.error, BadRequest) and \"Can't parse entities\" in str(context.error):\n",
    "         logger.warning(f\"Ignoring HTML parse error: {context.error}\")\n",
    "         return\n",
    "    logger.error(f\"Update {update} caused error {context.error}\", exc_info=context.error)\n",
    "\n",
    "\n",
    "# --- Main Function to Run the Bot ---\n",
    "def run_bot():\n",
    "    \"\"\"Sets up and runs the YouTube-focused Telegram bot.\"\"\"\n",
    "    global TELEGRAM_BOT_TOKEN, youtube_service, sentiment_pipeline_yt, sentence_tokenizer\n",
    "    if not TELEGRAM_BOT_TOKEN:\n",
    "        logger.critical(\"FATAL: Telegram Bot Token not found. Cannot start.\")\n",
    "        return\n",
    "\n",
    "    # Optional: Re-check critical components loaded in Cell 3 before starting\n",
    "    if not youtube_service: logger.warning(\"YouTube service not initialized at bot start.\")\n",
    "    if not sentiment_pipeline_yt: logger.warning(\"Sentiment pipeline not loaded at bot start.\")\n",
    "    if not sentence_tokenizer: logger.warning(\"Sentence tokenizer not loaded at bot start.\")\n",
    "\n",
    "\n",
    "    builder = Application.builder().token(TELEGRAM_BOT_TOKEN)\n",
    "    application = builder.build()\n",
    "\n",
    "    # Register command handlers (YouTube only)\n",
    "    application.add_handler(CommandHandler(\"start\", start))\n",
    "    application.add_handler(CommandHandler(\"help\", help_command))\n",
    "    application.add_handler(CommandHandler(\"youtube\", handle_youtube))\n",
    "\n",
    "    application.add_error_handler(error_handler)\n",
    "\n",
    "    logger.info(\"Starting YouTube Analyzer bot polling in Colab...\")\n",
    "    print(\"--- Bot is starting (YouTube Analyzer ONLY) ---\")\n",
    "    print(\"--- Make sure API Key & Token are in Colab Secrets ---\")\n",
    "    print(\"--- Ensure Cells 1-6 have run successfully ---\")\n",
    "    print(\"Send commands to your bot in Telegram.\")\n",
    "    print(\"Interrupt the kernel (Stop button) to stop the bot.\")\n",
    "    try:\n",
    "        application.run_polling(allowed_updates=Update.MESSAGE)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nBot stopped manually.\")\n",
    "        logging.info(\"Bot stopped manually.\")\n",
    "    except Exception as e:\n",
    "         print(f\"\\nAn critical error occurred running the bot: {e}\")\n",
    "         logging.critical(f\"An critical error occurred running the bot application: {e}\", exc_info=True)\n",
    "    finally:\n",
    "         print(\"Bot polling finished.\")\n",
    "         logging.info(\"Bot polling finished.\")\n",
    "\n",
    "\n",
    "# --- Start the bot ---\n",
    "if __name__ == \"__main__\":\n",
    "    run_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IHn6SyPLwcX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15yjKFQWAhCm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sa0Psjg1AhE-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUvsS5ueAhG4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1PU1UYquZonXbmzUUlfSqK5yJYpLn9kXq",
     "timestamp": 1745225221002
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
